{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import string\n",
    "import html\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "def ModelIt(file_names_input, root_dir, downloads_dir):\n",
    "    print(root_dir, downloads_dir)\n",
    "    root_dir = root_dir\n",
    "    downloads_dir = downloads_dir\n",
    "\n",
    "\n",
    "    if root_dir[-1] != '/':\n",
    "        root_dir = root_dir + '/'\n",
    "    if downloads_dir[-1] != '/':\n",
    "        downloads_dir = downloads_dir + '/'\n",
    "\n",
    "    if os.path.exists('root_dir'):\n",
    "        root_dir_pickled = pickle.load(open('root_dir', 'rb'))\n",
    "        if root_dir_pickled != root_dir:\n",
    "            model_train(root_dir)\n",
    "            symbolic_link_create(root_dir,downloads_dir)\n",
    "    else:\n",
    "        model_train(root_dir)\n",
    "\n",
    "\n",
    "    if os.path.exists('downloads_dir'):\n",
    "        downloads_dir_pickled = pickle.load(open('downloads_dir', 'rb'))\n",
    "        if downloads_dir_pickled != downloads_dir:\n",
    "            symbolic_link_create(root_dir,downloads_dir)\n",
    "    else:\n",
    "        symbolic_link_create(root_dir,downloads_dir)\n",
    "\n",
    "\n",
    "    pickle.dump(root_dir, open('root_dir', 'wb'))\n",
    "    pickle.dump(downloads_dir, open('downloads_dir', 'wb'))\n",
    "\n",
    "    path_pred = model_predict(file_names_input)\n",
    "    return path_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def symbolic_link_create(root_dir, downloads_dir):\n",
    "    dst = downloads_dir + 'symlink_folder'\n",
    "    if os.path.exists(dst):\n",
    "        os.unlink(dst)\n",
    "    os.symlink(root_dir, dst)\n",
    "    print(root_dir,'\\n',dst)\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def model_train(root_dir):\n",
    "    if root_dir == '':\n",
    "        return\n",
    "\n",
    "    root_dir = root_dir\n",
    "    data = []\n",
    "    for filename in glob.iglob(root_dir + '**/*', recursive=True):\n",
    "         data.append(os.path.relpath(filename, root_dir))\n",
    "    #print(data)\n",
    "    data_split = list(map(lambda x: os.path.normpath(x).split(os.sep), data))\n",
    "    data_drop_single = list(filter(lambda x: len(x)>1, data_split)) #change to len(x)>1 since solver needs >=2 classes!!!!\n",
    "    file_name = list(map(lambda x: x[-1], data_drop_single))\n",
    "    file_name_html = [html.unescape(x) for x in file_name]\n",
    "    file_name_underscore = [x.lower().replace(\"_\", \" \") for x in file_name_html]\n",
    "    file_name_word_split = [re.findall(r\"[\\w']+\", x) for x in file_name_underscore]\n",
    "    file_name_final = [' '.join(x) for x in file_name_word_split]\n",
    "\n",
    "    file_labels = list(map(lambda x: x[:-1], data_drop_single))\n",
    "    df_labels = pd.DataFrame(data = file_labels)\n",
    "    df_paths = pd.DataFrame(data = df_labels[0])\n",
    "    for i in range(1, df_labels.shape[1]):\n",
    "        df_paths[i] = df_labels[i]\n",
    "\n",
    "        df_paths.loc[pd.notna(df_labels[i]), i] = df_paths[i-1].map(str) + '/' + df_paths[i]\n",
    "\n",
    "    df_paths.replace('None', np.nan, inplace=True)\n",
    "    df_paths[-1] = 'dummy'\n",
    "    df_paths = df_paths.astype('category', copy = False)\n",
    "\n",
    "    # count_vect = CountVectorizer(min_df=8, encoding='latin-1', \\\n",
    "    #                 ngram_range=(1, 2), stop_words='english')\n",
    "    #\n",
    "    # X_train_counts = count_vect.fit_transform(file_name_final)\n",
    "    # features = X_train_counts.toarray()\n",
    "\n",
    "    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=8, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "    features = tfidf.fit_transform(file_name_final).toarray()\n",
    "\n",
    "    depth = df_paths.shape[1]\n",
    "    folder_each_level = [df_paths[x].cat.categories.tolist() for x in range(-1,df_paths.shape[1]-1)]\n",
    "\n",
    "    #print(folder_each_level)\n",
    "\n",
    "    clf = [[LogisticRegression(random_state = 0) for _ in range(len(folder_each_level[x]))] \\\n",
    "           for x in range(len(folder_each_level))]\n",
    "\n",
    "    for i in range(len(folder_each_level)-1):\n",
    "        for j in range(len(folder_each_level[i])):\n",
    "\n",
    "            # input control\n",
    "            ind = ((df_paths[i-1] == folder_each_level[i][j]) & (df_paths[i].notna())) # remove NAN paths at the same time\n",
    "\n",
    "            # category / labels control: next col of df_paths\n",
    "            features_level = features[ind] # just pick the corresponding entries\n",
    "            labels_level = np.asarray(df_paths[i][ind], dtype=\"str\")\n",
    "\n",
    "            if len(np.unique(labels_level)) > 1 and len(labels_level)>5:\n",
    "                clf[i][j] = LogisticRegression(random_state = 0)\n",
    "                clf[i][j].fit(features_level, labels_level)\n",
    "\n",
    "    pickle.dump(clf, open('clf_folder_picker', 'wb'))\n",
    "    #pickle.dump(count_vect, open('count_vect', 'wb'))\n",
    "    pickle.dump(folder_each_level, open('folder_each_level', 'wb'))\n",
    "    pickle.dump(tfidf, open('tfidf', 'wb'))\n",
    "\n",
    "def model_predict(file_names_input):\n",
    "    prob_threshold = 0.8\n",
    "\n",
    "    clf = pickle.load(open('clf_folder_picker', 'rb'))\n",
    "    #count_vect = pickle.load(open('count_vect', 'rb'))\n",
    "    tfidf = pickle.load(open('tfidf','rb'))\n",
    "    folder_each_level = pickle.load(open('folder_each_level', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "#     folder_each_level = [df_paths[x].cat.categories.tolist() for x in range(-1,df_paths.shape[1]-1)]\n",
    "\n",
    "    preds = []\n",
    "    for file_name_input in [file_names_input]:\n",
    "        #vect_name = count_vect.transform([file_name_input])\n",
    "        vect_name = tfidf.transform([file_name_input])\n",
    "        y_pred =  ['' for _ in range(len(folder_each_level)-1)]\n",
    "        prob = [0 for _ in range(len(folder_each_level)-1)]\n",
    "        prob_total = 1\n",
    "\n",
    "        current_folder = 'dummy'\n",
    "        for i in range(len(folder_each_level)-1):\n",
    "            for j in range(len(folder_each_level[i])):\n",
    "#                 print(i,j)\n",
    "\n",
    "                if current_folder != folder_each_level[i][j]: #make sure it is the right subfolder\n",
    "                    continue\n",
    "\n",
    "#                 print('folder_each_level:',folder_each_level[i][j])\n",
    "#                 print('current_folder:', current_folder)\n",
    "\n",
    "                try:\n",
    "                    clf[i][j].predict(vect_name)\n",
    "                except NotFittedError as e:\n",
    "                    #print('error')\n",
    "                    break\n",
    "\n",
    "                y_pred[i] = clf[i][j].predict(vect_name)[0]\n",
    "                current_folder = y_pred[i]\n",
    "                prob[i] = np.max(clf[i][j].predict_proba(vect_name))\n",
    "                prob_total *= prob[i]\n",
    "\n",
    "#                     print(y_pred[i], prob[i], prob_total,'\\n')\n",
    "\n",
    "\n",
    "\n",
    "            if prob_total < prob_threshold:\n",
    "                break\n",
    "\n",
    "        # if i > 0 and back up a level if prob_total < prob_threshold:\n",
    "        if (i > 0) and (prob_total < prob_threshold):\n",
    "            pred_folder = y_pred[i-1]\n",
    "            level = i\n",
    "        else:\n",
    "            pred_folder = y_pred[i]\n",
    "            level = i+1\n",
    "\n",
    "        # when there is no pretrained clf available at this level\n",
    "        if pred_folder == '':\n",
    "            #print(i, j, current_folder,'\\n', y_pred[i-2], y_pred[i-1])\n",
    "            pred_folder = current_folder\n",
    "        preds.append((pred_folder, level))\n",
    "    print(file_name_input)\n",
    "    path_pred = preds[0][0]\n",
    "    return 'symlink_folder' + '/' +path_pred + '/' + file_name_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
